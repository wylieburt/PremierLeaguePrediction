This confirmed the best accuracy score of 1.0.  Trying different hyperparameters would be ridiculous.  So, I started to think usually, I would see these results if I had not specified the stopping criteria in the form of the hyperparameters.  It must be something else.  I turned back the data.  Yes, I still needed to do feature selection, but there was something else.

I had inadvertantly included features that directly related to the output variable.  In otherwords, these input variables could vary well be the output variable.  This is called data leakage.  I found the culprets and removed them.  After reprocessing, the grid search found different parameters to use number of decisions trees is now 500, max depth will be 7, and max features is 11. The results showed a better confusion matrix - encouraging!
